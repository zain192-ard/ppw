{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Page PTA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "F2m5DW1_Sqf7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memulai crawl dari: https://pta.trunojoyo.ac.id/\n",
      "Domain target: pta.trunojoyo.ac.id\n",
      "Batas halaman: 100\n",
      "\n",
      "[1/100] Crawling: https://pta.trunojoyo.ac.id/\n",
      "  -> Gagal (HEAD): https://pta.trunojoyo.ac.id/: 403 Client Error: Forbidden for url: https://pta.trunojoyo.ac.id/\n",
      "\n",
      "Crawl selesai. Total 1 halaman di-visit.\n",
      "Total 0 link (edge) ditemukan.\n",
      "Tidak ada edge yang ditemukan. DataFrame akan kosong.\n",
      "\n",
      "Dataset akhir kosong, tidak ada file CSV yang disimpan.\n",
      "\n",
      "--- Selesai ---\n",
      "Total waktu eksekusi: 0 menit 0 detik (0.13 detik)\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, urlparse\n",
    "from collections import deque  # Antrian (queue) yang efisien\n",
    "import time\n",
    "import sys\n",
    "\n",
    "def build_pagerank_dataset(start_url, max_pages=50, delay=0.1):\n",
    "    \"\"\"\n",
    "    Melakukan crawling multi-halaman untuk membangun edge list untuk PageRank.\n",
    "    \n",
    "    :param start_url: URL untuk memulai crawl.\n",
    "    :param max_pages: Jumlah maksimum halaman yang akan di-crawl.\n",
    "    :param delay: Waktu tunggu antar-request (dalam detik) agar tidak membebani server.\n",
    "    :return: DataFrame pandas dengan kolom ['Halaman Sumber', 'Link Keluar (Internal)'].\n",
    "    \"\"\"\n",
    "    \n",
    "    pages_to_visit = deque([start_url])\n",
    "    visited_pages = set()\n",
    "    edge_list = []\n",
    "    \n",
    "    # ==================================================================\n",
    "    # === PERBAIKAN LOGIKA TRY/EXCEPT DI SINI ===\n",
    "    # ==================================================================\n",
    "    try:\n",
    "        # 1. Ambil hostname dulu\n",
    "        hostname = urlparse(start_url).hostname\n",
    "        \n",
    "        # 2. Cek apakah hostname valid SEBELUM di .replace()\n",
    "        if not hostname:\n",
    "            raise ValueError(\"Hostname tidak ditemukan. Pastikan URL diawali 'https://' atau 'http://'\")\n",
    "        \n",
    "        # 3. Baru lakukan .replace()\n",
    "        base_hostname = hostname.replace('www.', '')\n",
    "        \n",
    "    except (ValueError, AttributeError) as e:\n",
    "        # Sekarang block ini bisa menangkap 'AttributeError' (jika hostname None)\n",
    "        # atau 'ValueError' yang kita buat\n",
    "        print(f\"Error: URL awal tidak valid -> '{start_url}'\")\n",
    "        print(f\"Detail: {e}\")\n",
    "        return pd.DataFrame(columns=[\"Halaman Sumber\", \"Link Keluar (Internal)\"]) \n",
    "    # ==================================================================\n",
    "\n",
    "    print(f\"Memulai crawl dari: {start_url}\")\n",
    "    print(f\"Domain target: {base_hostname}\")\n",
    "    print(f\"Batas halaman: {max_pages}\\n\")\n",
    "\n",
    "    IGNORED_EXTENSIONS = (\n",
    "        '.png', '.jpg', '.jpeg', '.gif', '.svg', '.bmp', '.tiff', '.webp',\n",
    "        '.pdf', '.doc', '.docx', '.xls', '.xlsx', '.ppt', '.pptx',\n",
    "        '.zip', '.rar', '.gz', '.tar', '7z',\n",
    "        '.mp4', '.mkv', '.avi', '.mov', '.mp3', '.wav', '.ogg',\n",
    "        '.css', '.js', '.xml', '.json', '.csv'\n",
    "    )\n",
    "\n",
    "    while pages_to_visit and len(visited_pages) < max_pages:\n",
    "        current_url = pages_to_visit.popleft()\n",
    "        \n",
    "        if current_url in visited_pages:\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            cleaned_url_path = urlparse(current_url).path\n",
    "        except ValueError:\n",
    "            print(f\"  -> [SKIP] URL tidak valid: {current_url}\")\n",
    "            continue \n",
    "\n",
    "        if cleaned_url_path.lower().endswith(IGNORED_EXTENSIONS):\n",
    "            print(f\"  -> [SKIP] Mengabaikan file (dari ekstensi): {current_url}\")\n",
    "            continue\n",
    "            \n",
    "        visited_pages.add(current_url)\n",
    "        print(f\"[{len(visited_pages)}/{max_pages}] Crawling: {current_url}\")\n",
    "        \n",
    "        try:\n",
    "            time.sleep(delay)\n",
    "            \n",
    "            try:\n",
    "                head_response = requests.head(current_url, timeout=3, allow_redirects=True)\n",
    "                head_response.raise_for_status() \n",
    "                content_type = head_response.headers.get('Content-Type', '')\n",
    "                \n",
    "                if 'text/html' not in content_type:\n",
    "                    print(f\"  -> [SKIP] Mengabaikan tipe konten non-HTML: {content_type}\")\n",
    "                    continue\n",
    "            except requests.exceptions.RequestException as head_err:\n",
    "                if 'head_response' in locals() and head_response.status_code >= 400:\n",
    "                     print(f\"  -> Gagal (HEAD): {current_url}: {head_err}\")\n",
    "                     continue\n",
    "                pass\n",
    "\n",
    "            response = requests.get(current_url, timeout=5)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            final_content_type = response.headers.get('Content-Type', '')\n",
    "            if 'text/html' not in final_content_type:\n",
    "                 print(f\"  -> [SKIP] Mengabaikan tipe konten non-HTML (setelah GET): {final_content_type}\")\n",
    "                 continue\n",
    "            \n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            links = soup.find_all('a', href=True)\n",
    "            \n",
    "            for link in links:\n",
    "                href = link['href']\n",
    "                \n",
    "                if href.startswith(('#', 'javascript:', 'mailto:', 'tel:')) or not href.strip():\n",
    "                    continue\n",
    "                \n",
    "                absolute_url = urljoin(current_url, href)\n",
    "                absolute_url = absolute_url.split('#')[0]\n",
    "\n",
    "                try:\n",
    "                    link_hostname = urlparse(absolute_url).hostname\n",
    "                    if link_hostname and link_hostname.endswith(base_hostname):\n",
    "                        edge_list.append([current_url, absolute_url])\n",
    "                        if absolute_url not in visited_pages:\n",
    "                            pages_to_visit.append(absolute_url)\n",
    "                except ValueError:\n",
    "                    continue\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"  -> Gagal mengakses (GET) {current_url}: {str(e)}\")\n",
    "    \n",
    "    print(f\"\\nCrawl selesai. Total {len(visited_pages)} halaman di-visit.\")\n",
    "    print(f\"Total {len(edge_list)} link (edge) ditemukan.\")\n",
    "    \n",
    "    if not edge_list:\n",
    "        print(\"Tidak ada edge yang ditemukan. DataFrame akan kosong.\")\n",
    "        return pd.DataFrame(columns=[\"Halaman Sumber\", \"Link Keluar (Internal)\"])\n",
    "    \n",
    "    df = pd.DataFrame(edge_list, columns=[\"Halaman Sumber\", \"Link Keluar (Internal)\"])\n",
    "    # df = df[df['Halaman Sumber'] != df['Link Keluar (Internal)']]\n",
    "    df = df.drop_duplicates().reset_index(drop=True)\n",
    "    \n",
    "    print(f\"Dataset akhir memiliki {len(df)} edge.\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# --- Bagian Utama untuk Menjalankan Kode ---\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "\n",
    "    # ==================================================================\n",
    "    target_url = \"https://pta.trunojoyo.ac.id/\" # <-- Yang benar pakai 'https://'\n",
    "    # ==================================================================\n",
    "    \n",
    "    batas_halaman = 100\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    pagerank_df = build_pagerank_dataset(target_url, max_pages=batas_halaman, delay=0.1)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    duration = end_time - start_time\n",
    "    \n",
    "    if not pagerank_df.empty:\n",
    "        print(\"\\n--- Contoh Hasil Dataset (Head) ---\")\n",
    "        print(pagerank_df.head(10))\n",
    "        \n",
    "        output_filename = \"pagerank500_edges_pta.csv\"\n",
    "        pagerank_df.to_csv(output_filename, index=False)\n",
    "        print(f\"\\nFile berhasil disimpan ke {output_filename}\")\n",
    "    else:\n",
    "        print(\"\\nDataset akhir kosong, tidak ada file CSV yang disimpan.\")\n",
    "\n",
    "    print(f\"\\n--- Selesai ---\")\n",
    "    minutes = int(duration // 60)\n",
    "    seconds = int(duration % 60)\n",
    "    print(f\"Total waktu eksekusi: {minutes} menit {seconds} detik ({duration:.2f} detik)\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}